{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###prerequisite :\n",
    "##download ffmpeg\n",
    "#add it to paath by cmd admin :\n",
    "#setx /M PATH \"%PATH%;C:\\Users\\thiba\\OneDrive - Ecole de l'air\\USAFA IA\\Vcode\\Aero_voice_rec\\ffmpeg\\bin\"\n",
    "\n",
    "#download https://visualstudio.microsoft.com/fr/thank-you-downloading-visual-studio/?sku=Community&channel=Release&version=VS2022&source=VSLandingPage&cid=2030&passive=false\n",
    "#for the use of LM\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from datasets import load_dataset, load_metric, Audio\n",
    "import tabulate\n",
    "import torch\n",
    "from transformers import AutoModelForCTC, Wav2Vec2Processor, Wav2Vec2ProcessorWithLM\n",
    "import torchaudio.functional as F\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Jzuluaga/wav2vec2-large-960h-lv60-self-en-atc-atcosim were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at Jzuluaga/wav2vec2-large-960h-lv60-self-en-atc-atcosim and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 3832.16it/s]\n",
      "Only 0 unigrams passed as vocabulary. Is this small or artificial data?\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipes = {\n",
    "    \"Jzuluaga/wav2vec2-large-960h-lv60-self-en-atc-atcosim\": pipeline(\"automatic-speech-recognition\", model=\"Jzuluaga/wav2vec2-large-960h-lv60-self-en-atc-atcosim\"),\n",
    "    \"facebook/wav2vec2-base-960h\":                           pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\"),\n",
    "    \"scy0208/whisper-aviation-base\":                         pipeline(\"automatic-speech-recognition\", model=\"scy0208/whisper-aviation-base\"),\n",
    "    \"billodal/whisper-small-atc\":                            pipeline(\"automatic-speech-recognition\", model=\"billodal/whisper-small-atc\", return_timestamps=False)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thiba\\OneDrive - Ecole de l'air\\USAFA IA\\Vcode\\.env\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\thiba\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "pipe_summerize = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7878\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI model of an aeronotical speech. Here's the transcript.\n",
      "Here's the transcript by an AI model of an aeronotical speech.\n",
      " AI model of an aeronotical speech. Here's the transcript.\n"
     ]
    }
   ],
   "source": [
    "def summarize_me(text_sent):\n",
    "    result = pipe_summerize(text_sent, max_length=50, min_length=10, do_sample=False)[0]['summary_text']\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "def transcribe_audio(audio_file_path):\n",
    "    transcriptions = {}\n",
    "\n",
    "    for name, pipe in pipes.items():\n",
    "        try:\n",
    "            # Assurez-vous d'accéder au résultat de transcription correctement\n",
    "            full_text  = pipe(audio_file_path)['text']\n",
    "            summerized = summarize_me('Here\\'s the transcript by an AI model of an aeronotical speech : ' + full_text)\n",
    "\n",
    "            transcriptions[name] = full_text\n",
    "            transcriptions[\"Summerized \" + name] = summerized\n",
    "\n",
    "        except Exception as e:\n",
    "            transcriptions[name] = f\"Erreur de transcription: {str(e)}\"\n",
    "            transcriptions[\"Summarized \" + name] = f\"Erreur de résumé: {str(e)}\"\n",
    "        \n",
    "    return [transcriptions[key] for key in transcriptions.keys()]\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Création des sorties en tant que liste de Textbox, chaque Textbox étiquetée par le nom du modèle\n",
    "output_components = []\n",
    "for name in pipes.keys():\n",
    "    output_components.append(gr.Textbox(label=f\"Transcription by {name}\"))\n",
    "    output_components.append(gr.Textbox(label=f\"Summarized (BERT) of {name}\"))\n",
    "\n",
    "# Configuration de l'interface Gradio\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe_audio,\n",
    "    inputs=gr.Audio(label=\"Upload MP3 File\", type=\"filepath\"),\n",
    "    outputs=output_components,  # Passer la liste de Textbox au lieu d'un dictionnaire\n",
    "    title=\"Audio Transcription Service, Specialized in Aeronautical Speech\",\n",
    "    description=\"Upload an MP3 file to get its transcription using various fine-tuned models.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
